---
title: "Random forest"
format: html
---
### Random forest

# Libraries

```{r}
pacman::p_load(tidymodels, tidyverse, future, doRNG, GGally, tictoc, pdp, foreach, doParallel, ranger, caret, GGally, ggcorrplot, corrr, hstats)
```

## Data

Define predictors, exclusions and responses for further analyses

```{r}
source("predictors_and_responses.r")
names(readRDS("Data/wdpa.rds"))
```

## Model settings

```{r}
# Re-import dataset
exclude <- c("decimallongitude", "decimallatitude", "built_space", "realm", "X", "Y", "Z")

data <- readRDS("Data/wdpa.rds") %>%
    select(all_of(c("WDPAID", responses, predictors[!predictors %in% exclude]))) %>%
    recipe(~.) %>%
    step_naomit() %>%
    step_filter(area > 1, continent != "Antarctica") %>%
    prep() %>%
    bake(new_data = NULL)

# Remove NAs
data <- na.omit(data)

# Check that the pre-processing didn't deleted all the data
if (nrow(data) > 0) {
    nrow(data)
} else {
    stop("Data preprocessing resulted in an empty dataset.")
}

# Check NAs per column
sapply(data, function(x) sum(is.na(x)))

set.seed(20240626)
split_prop <- 3 / 4
ml_engine <- "ranger"
ml_mode <- "regression"
ml_metrics <- metric_set(rmse, rsq, mae)

my_sapply <- function(X, FUN, ...) {
    sapply(X, FUN, ..., simplify = FALSE, USE.NAMES = TRUE)
}

```

# Create the model

```{r}
ml_model <-
    # specify that the model is a random forest
    rand_forest() %>%
    # Parameters to be tuned
    set_args(
        mtry = tune(),
        trees = tune(),
        min_n = tune()
    ) %>%
    # Select the modeling engine
    set_engine(engine = ml_engine, importance = "permutation") %>%
    # Select between regression and binary classification
    set_mode(ml_mode)
```

## Tuning parameters:

```{r}
npred <- length(predictors[!predictors %in% exclude])
tuning_params <- expand.grid(
    mtry = floor(c(sqrt(npred) / 2, sqrt(npred), 2 * sqrt(npred), npred / 3)), # number predictors
    trees = c(100, 10 * npred, seq(500, 2000, by = 500)), # Number of trees
    min_n = c(2, 5, 10, 15) # Minimum number of samples required to split an
) %>% unique()

nrow(tuning_params)
```

## Preparing training and testing data

```{r}
ml_data <- select(data, any_of(c(responses, predictors)))
set.seed(2019820787)
ml_split <- initial_split(ml_data, prop = split_prop)

ml_split

ml_train <- training(ml_split)
ml_test <- testing(ml_split)
ml_kfold <- vfold_cv(ml_train)
```

## Create recipe

```{r}
# https://recipes.tidymodels.org/articles/Ordering.html

ml_recipes <- my_sapply(responses, function(y) {
    if (y %in% c("elevation_cv","n_climates","shannon_climates")) {
        exclude <- c(exclude, "edge_prop")
    }
    pred <- paste(predictors[!predictors %in% exclude],
        collapse = "+"
    )
    formula <- as.formula(str_glue("{y} ~ {pred}"))
    print(formula)
    ml_recipe <- recipe(formula, data = ml_train) %>%
        # Pre-processing steps
        ## Transform numeric predictors
        # step_YeoJohnson(all_numeric()) %>%
        step_log(area, base = 10) %>%
        # Handle unseen levels in categorical variables
        step_novel(all_nominal()) %>%
        ## Recode categorical variables
        step_dummy(all_nominal()) %>%
        ## Standardize all numeric columns
        # step_normalize(all_numeric()) %>%
        # Remove NAs
        step_naomit()

    return(ml_recipe)
})
```

## Create a workflow

```{r}
ml_workflows <- my_sapply(names(ml_recipes), function(recipe) {
    ml_workflow <- workflow() %>%
        # add the recipe
        add_recipe(ml_recipes[[recipe]]) %>%
        # add the model
        add_model(ml_model)
})
```

## Tune parameters

```{r}
ntuning <- nrow(tuning_params)
nlimit <- 100
plan("future::multisession",
    workers = ifelse(ntuning < nlimit, ntuning, nlimit)
)

# extract results
ml_tune_results <- my_sapply(names(ml_workflows), function(workflow) {
    try({
        ml_workflows[[workflow]] %>%
            tune_grid(
                # Cross-validation data object
                resamples = ml_kfold,
                # Grid of hyperparameters
                grid = tuning_params,
                # Metrics to evaluate
                metrics = ml_metrics
            )
    })
})

saveRDS(ml_tune_results, "rds_backups/ml_tune_results.rds")
```

## Plot tunned parameters

```{r}
# Re-import tuning results
ml_tune_results <- readRDS("rds_backups/ml_tune_results.rds")

# Generate and display plots for each element in ml_tune_results
lapply(ml_tune_results, function(x) {
    try({
        autoplot(x) + ggtitle("Tuning Results")
    })
})
```

## Collect and evaluate results

```{r}
#| eval: true
ml_tune_results <- my_sapply(names(ml_tune_results), function(x) {
    try({
        select_best(ml_tune_results[[x]], metric = "rmse")
    })
})

print(ml_tune_results)
```

## Finalize workflow

```{r}
#| eval: true
for (x in 1:length(ml_workflows)) {
    try({
        ml_workflows[[x]] <- finalize_workflow(
            ml_workflows[[x]],
            ml_tune_results[[x]]
        )
    })
}
```

# Fit the final model to the training and testing data

```{r}
#| eval: true
ml_fits <- mclapply(ml_workflows, function(workflow) {
    try({
        workflow %>%
            # fit on the training set and evaluate on test set
            last_fit(ml_split)
    })
}, mc.cores = 10)
```

check results

```{r}
test_performance <- mclapply(ml_fits, function(ml_fit) {
    try({
        ml_fit %>% collect_metrics()
    })
}, mc.cores = 10)

names(test_performance) <- responses

test_performance <- bind_rows(test_performance, .id = "Response")

test_performance <- test_performance %>%
    select(Response, .metric, .estimate) %>%
    pivot_wider(names_from = .metric, values_from = .estimate)

test_performance
```

### Variable importance plots

```{r}
pacman::p_load(vip, patchwork, dplyr, ggplot2, ggnewscale, ggpubr, tagger)

n_impvars <- 5

names(ml_fits) <- responses

extract_varimportance <- function(model, response, num_features = 15) {
    rsq <- test_performance %>%
        filter(Response == response) %>%
        pull(rsq)
    f_model <- extract_fit_parsnip(model)
    importance_scores <- vip(f_model$fit, num_features = num_features, geom = "col")$data
    rsq <- round(rsq, 2)
    importance_scores <- importance_scores %>%
        mutate(std_importance = (Importance * rsq) / sum(Importance)) %>%
        mutate(Response = str_glue("{response}~(R^2 == {rsq})")) # Add response variable for faceting
    return(importance_scores)
}

# Extract importance scores for all models and combine into a single dataframe
vip_data <- bind_rows(lapply(names(ml_fits), function(response) {
    extract_varimportance(ml_fits[[response]], response)
}))

# Plot variable importance with facets and merged x axes, colored by importance (scaled per facet)
vip_data <- vip_data %>%
    group_by(Response) %>%
    mutate(
        rank = rank(-std_importance, ties.method = "min"),
        fill_color = ifelse(rank <= n_impvars, "top5", "other")
    ) %>%
    ungroup()

levels <- lapply(responses, function(x) {
    vector <- unique(vip_data$Response)
    str_subset(vector, x) # Directly return matches as character vector
}) %>%
    unlist(use.names = FALSE) %>% # Combine all matches, remove list structure
    unique()

levels <- str_replace_all(levels, c(
    "cv_ndvi" = "CV-NDVI",
    "elevation_cv" = "CV-ELEV",
    "\\bn_climates" = "N-CLIM",
    "shannon_climates" = "DIV-CLIM",
    "\\bn_landcover" = "N-LC",
    "shannon_landcover" = "DIV-LC"
))

vip_data$Response <- str_replace_all(vip_data$Response, c(
    "cv_ndvi" = "CV-NDVI",
    "elevation_cv" = "CV-ELEV",
    "\\bn_climates" = "N-CLIM",
    "shannon_climates" = "DIV-CLIM",
    "\\bn_landcover" = "N-LC",
    "shannon_landcover" = "DIV-LC"
))

vip_data$Response <- factor(vip_data$Response, levels = levels)

vip_plot <- ggplot(vip_data, aes(x = reorder(Variable, std_importance), y = std_importance, fill = fill_color)) +
    geom_col() +
    coord_flip() +
    facet_wrap(~Response, 
        labeller = label_parsed,
        axes = "all",
        axis.labels = "margins") +
    scale_fill_manual(values = c("top5" = "forestgreen", "other" = "lightblue")) +
    labs(title = "", x = "", y = "") +
    tag_facets(tag_levels = "a", tag_prefix = "(", tag_suffix = ")") +
    expand_limits(y = -0.01) +
    theme_pubr() +
    theme(
        text = element_text(size = 14),
        legend.position = "none",
        strip.text = element_text(size = 14),
        strip.background = element_blank(),
        axis.title.x = element_blank(),
        strip.placement = "outside",
        tagger.panel.tag.text = element_text(size = 16, face = "bold")
    )

# Save the plot
ggsave("figures/faceted_variable_importance_plots.svg",
    plot = vip_plot, width = 19, height = 11.81, units = "in", dpi = 300, bg = "white"
)
```

