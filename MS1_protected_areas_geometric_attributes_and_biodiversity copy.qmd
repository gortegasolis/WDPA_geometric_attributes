---
title: "Relationship between geometric attributes of protected areas, habitat heterogeneity and biodiversity"
author: 
  - name: Gabriel Ortega
  - name: Petr Keil
  - name: Michela Perrone
  - name: Daniela Mellado
  - name: Manuele Bazzichetto
  - name: Carmen Soria
abstract: |
  Protected areas (PAs) design emphasizes shapes closer to a circle to reduce border effects. Kunin (1997) shows that plots with elongated shapes have more biodiversity. Kunin's findings are based on the principle that closer things are more similar, so elongated plots (ergo elongated PAs) should include things that are more distant between them, which are more likely to be different. While we cannot record species richness (or another diversity metric) worldwide simultaneously and with spatial accuracy, metrics derived from Earth Observation have been proposed as alternative proxies for biodiversity monitoring. The Spectral Variation Hypothesis proposes that geographic areas with high heterogeneity in remote images present high environmental heterogeneity, and therefore, support more biodiversity. 
format: 
  html: 
    toc: true
    toc-depth: 2
    number-sections: true
    code-fold: true
execute:
  eval: false
---

# Introduction

-Protected areas purposes

-Spatial design of protected areas Protected areas (PAs) design emphasizes shapes closer to a circle to reduce border effects.

-Distance decay principle Kunin (1997) shows that plots with elongated shapes have more biodiversity. Kunin's findings are based on the principle that closer things are more similar, so elongated plots (ergo elongated PAs) should include things that are more distant between them, which are more likely to be different. While we cannot record species richness (or another diversity metric) worldwide simultaneously and with spatial accuracy, metrics derived from Earth Observation have been proposed as alternative proxies for biodiversity monitoring. The Spectral Variation Hypothesis proposes that geographic areas with high heterogeneity in remote images present high environmental heterogeneity, and therefore, support more biodiversity.

-Relationship between geometric attributes and habitat heterogeneity The Spectral Variation Hypothesis proposes that geographic areas with high heterogeneity in remote images present high environmental heterogeneity, and therefore, support more biodiversity.

-Here we address the relationship between geometric attributes and habitat heterogeneity in protected areas worldwide.

-The fundamental principle behind all our hypotheses is the distance decay of similarity (Tobler’s law, Tobler XXX, Nekola & White 2001), which states that closer locations are more similar in environmental conditions and species composition than distant areas.

# Hypotheses

1.  **Heterogeneity increases with area**: Larger protected areas encompass a broader range of habitats by including more distant locations, thereby increasing their habitat heterogeneity due to the principle of distance decay of similarity.
2.  **Elongated PAs have more habitat heterogeneity**: Given constant area, elongated PAs cover longer environmental gradients than round PAs, increasing habitat heterogeneity.
3.  **Fragmented PAs harbor more heterogeneity**: Given constant area, fragmented PAs (measured as the number of polygons) are expected to have higher heterogeneity.
4.  **Shape complexity increases heterogeneity**: Given constant area, PAs with higher shape complexity are expected to have more heterogeneity.
5.  **North-South extent increases heterogeneity**: Habitat heterogeneity increases with increasing North-South extent, as PAs capture the global latitudinal gradient of temperature and precipitation. This effect is detectable above a threshold value of extent.

We expect some of these metrics/hypotheses to be correlated with each other, and it may not be possible to separate their effects due to fundamental dependencies (Sizling et al. 2024).

# Methods

## Step 1: Protected areas dataset

We used the January 2025 version of the World Database of Protected Areas (WDPA). We restricted the analyses to terrestrial protected areas with area higher than 1 square kilometer and whose designation status was one of the following: "Designated", "Inscribed", "Adopted" or "Established". The WDPA includes overlapping protected areas resulting from alternative designations or protections schemas by local and international institutions. For instance, part of a national park could be also a natural monument and be listed twice in the database. To avoid problems with overlapping areas we dissolved all overlapping spatial poligons. Greenland was removed from the dataset because of its extremely big size and northernmost location.

```{bash}
mkdir -p /tmp/data
unzip data.zip -d /tmp/data/
```

### Data preparation

```{r}
pacman::p_load(tidyverse)
climate <- list.files("/tmp/data/", full.names = T, pattern = "climate_analysis") %>%
    lapply(., read_csv) %>%
    data.table::rbindlist(fill = T) %>%
    select(-1, -.geo)
elevation <- list.files("/tmp/data/", full.names = T, pattern = "elevation_stats") %>%
    lapply(., read_csv) %>%
    data.table::rbindlist(fill = T) %>%
    select(-1, -.geo)
ndvi <- list.files("/tmp/data/", full.names = T, pattern = "mean_sd_NDVI") %>%
    lapply(., read_csv) %>%
    data.table::rbindlist(fill = T) %>%
    select(-1, -.geo)
coords <- list.files("/tmp/data/", full.names = T, pattern = "WDPA_lat") %>% read_csv(col_select = -4)
data <- list.files("/tmp/data/", full.names = T, pattern = "^MOBI_") %>%
    read_csv() %>%
    rename(WDPAID = wdpaid)
```

#### Climate

```{r}
# Process the climate data to extract and calculate relevant metrics
climate <- climate %>%
    mutate(
        # Extract the minimum bucket value from the climate histogram using regex
        bucketMin = as.numeric(str_extract(climate_histogram, "bucketMin=([0-9.]+)") %>% str_extract("[0-9.]+")),

        # Extract the bucket width from the climate histogram using regex
        bucketWidth = as.numeric(str_extract(climate_histogram, "bucketWidth=([0-9.]+)") %>% str_extract("[0-9.]+")),

        # Extract the histogram values as a numeric list using regex and string manipulation
        histogram = str_extract(climate_histogram, "(?<=histogram\\=)\\[.*?\\]") %>%
            str_remove_all("\\[|\\]") %>%
            str_split(",\\s*") %>%
            map(as.numeric),

        # Calculate the number of climate categories (bins) in the histogram
        n_climates = map_dbl(histogram, length),

        # Convert histogram values to numeric for further processing
        histogram_numeric = map(histogram, as.numeric)
    ) %>%
    group_by(WDPAID) %>%
    summarise(
        # Calculate the mean values for numeric columns (bucketMin, bucketWidth, n_climates)
        across(c(bucketMin, bucketWidth, n_climates), mean),

        # Compute the element-wise average of histogram lists across grouped data
        histogram_numeric = list(
            reduce(map(histogram_numeric, ~ {
                vec <- .x
                length(vec) <- max(lengths(histogram_numeric)) # Ensure all vectors have the same length
                replace_na(vec, 0) # Replace NA values with 0
            }), `+`) / n()
        )
    ) %>%
    mutate(
        # Calculate Shannon diversity index for each histogram
        shannon = map_dbl(histogram_numeric, ~ {
            counts <- .x[.x > 0] # Filter out zero counts
            if (length(counts) == 0) {
                return(0)
            } # Return 0 if no counts exist
            p <- counts / sum(counts) # Calculate proportions
            -sum(p * log(p)) # Compute Shannon index
        }),

        # Calculate Simpson diversity index for each histogram
        simpson = map_dbl(histogram_numeric, ~ {
            counts <- .x[.x > 0] # Filter out zero counts
            if (length(counts) == 0) {
                return(0)
            } # Return 0 if no counts exist
            p <- counts / sum(counts) # Calculate proportions
            1 - sum(p^2) # Compute Simpson index
        }),

        # Calculate evenness index for each histogram
        evenness = map_dbl(histogram_numeric, ~ {
            counts <- .x[.x > 0] # Filter out zero counts
            if (length(counts) == 0) {
                return(1)
            } # Return 1 if no counts exist
            p <- counts / sum(counts) # Calculate proportions
            H <- -sum(p * log(p)) # Compute Shannon index
            S <- length(counts) # Number of categories
            ifelse(S > 1, H / log(S), 1) # Compute evenness index
        })
    )
```

#### Elevation

```{r}
elevation <- elevation %>%
    group_by(WDPAID) %>%
    summarise(across(everything(), mean)) %>%
    mutate(
        elevation_range = elevation_max - elevation_min,
        elevation_cv = elevation_stdDev / elevation_mean
    )
```

#### NDVI - EVI

```{r}
ndvi <- ndvi %>%
    select(-year) %>%
    group_by(WDPAID) %>%
    summarise(
        EVI_p90_mean = mean(EVI_p90_mean),
        NDVI_p90_mean = mean(NDVI_p90_mean),
        EVI_p90_stdDev = sqrt(mean(EVI_p90_stdDev^2)),
        NDVI_p90_stdDev = sqrt(mean(NDVI_p90_stdDev^2))
    ) %>%
    mutate(
        cv_evi = EVI_p90_stdDev / EVI_p90_mean,
        cv_ndvi = NDVI_p90_stdDev / NDVI_p90_mean
    )
```

#### Add continents and biomes
```{r}
pacman::p_load(sf)

# Convert coords to sf object
coords <- st_as_sf(coords, coords = c("X", "Y"))

st_crs(coords) <- 4326

# Load continent and biome data
system("unzip Data/continents.zip -d /tmp")
continents <- st_read("/tmp/continents.gdb")[, c("CONTINENT", "SHAPE")] %>%
    rename(continent = CONTINENT, geometry = SHAPE) %>%
    st_make_valid() %>%
    st_transform(., st_crs(coords))
biomes <- st_read("wwf_ecoregions/wwf_terr_ecos.shp")[, c("REALM", "geometry")] %>%
    rename(realm = REALM) %>%
    st_make_valid() %>%
    st_transform(., st_crs(coords))

# Add continents to coords
coords <- st_join(coords, continents, join = st_intersects)

# Add biomes to coords
coords <- st_join(coords, biomes, join = st_intersects)

# Add coordinates as X and Y columns and remove geometry
coords <- coords %>%
    mutate(X = st_coordinates(.)[, 1], Y = st_coordinates(.)[, 2],
    lon = st_coordinates(.)[, 1], lat = st_coordinates(.)[, 2]) %>%
    st_drop_geometry()
```


#### Latitude and longitude to cartesian coordinates

```{r}
pacman::p_load(GeoThinneR)

# Apply lat-long transformation into cartesian coordinates
geo2cartesian <- function(data, lat, lon) {
    xyz <- lon_lat_to_cartesian(lat = data[[lat]], lon = data[[lon]])
    data[[lon]] <- xyz[, 1] # First column is x
    data[[lat]] <- xyz[, 2] # Second column is y
    data$Z <- xyz[, 3] # Third column is z
    return(data)
}

coords <- geo2cartesian(coords, lat = "Y", lon = "X")
```


### Merge all datasets

```{r}
# Perform joins, handle missing values and calculate additional variables
data <- data %>%
    left_join(climate, by = "WDPAID") %>%
    left_join(elevation, by = "WDPAID") %>%
    left_join(ndvi, by = "WDPAID") %>%
    left_join(coords, by = "WDPAID") %>%
    drop_na() %>%
    mutate(ns_elongation = ns_length / ew_length) %>%
    slice_sample(n = 5000)

saveRDS(data, "/tmp/wdpa.rds")
```

## Step 2: Analysis

Define predictors and responses for further analyses
```{r}
predictors <- c("area", "elongationratio", "normcircularity", "numpolygons", "numholes", "fractaldimension", "maxlength", "continent", "realm", "X", "Y", "Z","lon","lat")

responses <- c("NDVI_p90_mean", "cv_ndvi", "n_climates", "shannon", "evenness", "elevation_range", "elevation_cv")
```

-   Paired correlations
```{r}
data <- readRDS("/tmp/wdpa.rds")

corr_plot_resp <- GGally::ggpairs(select(data, all_of(responses))) +
    theme_pubclean()

corr_plot_pred <- GGally::ggpairs(select(data, all_of(predictors))) +
    theme_pubclean()

ggsave(str_glue("figures/corrplot_resp.png"),
    plot = corr_plot_resp, width = 30, height = 30, units = "cm", bg = "white"
)

ggsave(str_glue("figures/corrplot_pred.png"),
    plot = corr_plot_pred, width = 30, height = 30, units = "cm", bg = "white"
)
```

-   Ordination with all attributes of protected areas.

```{r}
# Load required packages
pacman::p_load(ggplot2, tidyverse, tidymodels, ggforce, rstatix, ggrepel, EFAtools, adegenet, geosphere, ggpubr)

# Re-import dataset
data <- readRDS("/tmp/wdpa.rds") %>%
    select(all_of(c("WDPAID", responses, predictors))) %>%
    recipe(~.) %>%
    step_naomit() %>%
    step_filter(area > 1) %>%
    step_scale(all_numeric(), -lon, -lat) %>%
    step_YeoJohnson(all_numeric(), -lon, -lat) %>%
    prep() %>%
    bake(new_data = NULL) %>%
    cbind(., mahalanobis_distance(.)[, c("mahal.dist", "is.outlier")]) %>%
    filter(!is.outlier)

# Run Bartlett's test of sphericity (must be significant) and compute Kaiser-Meyer-Olkin criteria (must be above 0.5)
BARTLETT(select(data, !any_of(c("WDPAID","continent", "realm", "mahal.dist", "is.outlier"))))
KMO(select(data, !any_of(c("WDPAID","continent", "realm", "mahal.dist", "is.outlier"))))

# Prepare variables for spatial PCA
spca_vars <- c(predictors[!predictors %in% c("continent", "realm", "X","Y","Z","lon","lat","mahal.dist", "is.outlier")])
X <- as.matrix(select(data, all_of(spca_vars)))

# Use coordinates for spatial information
coords <- select(data, lon, lat) %>%
    st_as_sf(coords = c("lon", "lat"), crs = 4326) %>%
    st_transform(crs = "+proj=moll") %>%
    st_coordinates()

# Use chooseCN to create a spatial weights list (k-nearest neighbors)
nb <- chooseCN(coords, type = 6, k = 12, plot.nb = F, result.type = "listw")

# Run spatial PCA (adegenet::spca)
spca_res <- adegenet::spca(X, cn = nb, xy = coords, scannf = FALSE, nfposi = 4, nfnega = 4)

pca_summary <- summary(spca_res)

# Prepare data for ggplot: axes 1 and 2, with spatial filtering (e.g., color by continent or realm)
ordination_df <- data %>%
    mutate(
        sPCA1 = spca_res$li[,1],
        sPCA2 = spca_res$li[,2]
    )

# Calculate explained variance for each sPCA axis
explained_var <- round(100 * pca_summary$spca$var, 1)

# Calculate Moran's I for each sPCA axis (positive and negative)
library(spdep)
moran_list <- sapply(1:ncol(spca_res$li), function(i) {
    moran.mc(spca_res$li[,i], nb, 999)$statistic
})
moran_list <- round(moran_list, 2)

# Calculate variable loadings for sPCA axes 1 and 2
loadings <- as.data.frame(spca_res$c1[, 1:2])
loadings$variable <- rownames(loadings)
colnames(loadings)[1:2] <- c("sPCA1", "sPCA2")

# Scale loadings for plotting as arrows
arrow_scale <- 3 # adjust as needed for visual clarity
loadings$sPCA1 <- loadings$sPCA1 * arrow_scale
loadings$sPCA2 <- loadings$sPCA2 * arrow_scale

# Prepare axis labels with explained variance and Moran's *I*
xlab <- glue::glue("PCA1 (σ²={explained_var[1]}%, *I* = {moran_list[1]})")
ylab <- glue::glue("PCA 2 (σ²={explained_var[2]}%, *I* = {moran_list[2]})")

# Use stat_density_2d with lower alpha and fewer bins to emphasize broader, more scattered regions
loads_color <- "gray50"
pca_plot <- ggplot(ordination_df, aes(x = sPCA1, y = sPCA2, color = continent)) +
    geom_point(alpha = 0.5, size = 1) +
    stat_density_2d(
        aes(fill = after_stat(level)),
        geom = "polygon",
        alpha = 1,
        color = NA,
        bins = 100
    ) +
    scale_fill_viridis_c(option = "C") +
    geom_segment(
        data = loadings,
        aes(x = 0, y = 0, xend = sPCA1, yend = sPCA2),
        arrow = arrow(length = unit(0.25, "cm")),
        color = loads_color,
        inherit.aes = FALSE
    ) +
    geom_text_repel(
        data = loadings,
        aes(x = sPCA1, y = sPCA2, label = variable),
        color = loads_color,
        size = 3,
        hjust = 0.5, vjust = -0.5,
        inherit.aes = FALSE
    ) +
    theme_pubr() +
    labs(
        title = "",
        x = bquote(PCA[1]~"(σ²="*.(explained_var[1])*"%, " * italic(I) * "=" * .(moran_list[1]) * ")"),
        y = bquote(PCA[2]~"(σ²="*.(explained_var[2])*"%, " * italic(I) * "=" * .(moran_list[2]) * ")"),
        fill = "Density",
        color = "Continent"
    ) +
    guides(
        fill = guide_colorbar(title = "Density", title.position = "top", title.hjust = 0.5),
        color = guide_legend(title = "", title.position = "top", title.hjust = 0.5)
    ) +
    theme(legend.position = "bottom")

ggsave("figures/spca_plot.png", plot = pca_plot, width = 20, height = 22, units = "cm", bg = "white")
```    

-   **H1**: Test the relationship between elongation and CV-EVI (and CV-NDVI) while controlling for area.
-   **H2**: Test the marginal effect of length in the North-South axis.
-   **H3**: \[Details to be added\].
-   **H4**: \[Details to be added\].

### Random forest

# Libraries

```{r}
pacman::p_load(tidymodels, tidyverse, future, doRNG, GGally, tictoc, pdp, foreach, doParallel, ranger, caret, GGally, ggcorrplot, corrr)
```

## Data

## Model settings

```{r}
# Re-import dataset
data <- readRDS("/tmp/wdpa.rds") %>%
    na.omit() %>%
    dplyr::select(-matches("^intersecting|^bucket|histogram_numeric|stdDev$")) %>%
    filter(ns_length >= 1, ew_length >= 1) %>%
    mutate(ns_elongation = ns_length / ew_length)

split_prop <- 3 / 4
ml_engine <- "ranger"
ml_mode <- "regression"
ml_metrics <- metric_set(rmse, rsq, mae)

my_sapply <- function(X, FUN, ...) {
  sapply(X, FUN, ..., simplify = FALSE, USE.NAMES = TRUE)
}

```

## Tuning parameters:

```{r}
tuning_params <- expand.grid(
    mtry = seq(from = 2, to = length(predictors), by = 3), # number predictors
    trees = c(100, seq(250, 5000, by = 250)), # Number of trees
    min_n = c(2, 5) # Minimum number of samples required to split an
)

nrow(tuning_params)
```

## Data preparation

```{r}
#| eval: true
ml_data <- select(data, any_of(c(responses, predictors)))

# Correlation between predictors and responses
predictor_response_corr <- ml_data %>%
    select(all_of(c(predictors, responses))) %>%
    select(where(is.numeric)) %>% # Ensure only numeric columns are selected
    cor() %>%
    as.data.frame() %>%
    rownames_to_column(var = "Variable") %>%
    filter(Variable %in% predictors) %>%
    pivot_longer(-Variable, names_to = "Response", values_to = "Correlation") %>%
    arrange(desc(abs(Correlation))) %>%
    mutate(
        Response = factor(Response, levels = unique(Response)),
        Variable = factor(Variable, levels = unique(Variable))
    )

ggplot(predictor_response_corr, aes(x = Response, y = Variable, fill = Correlation)) +
    geom_tile() +
    geom_text(
        aes(label = ifelse(abs(Correlation) >= 0.5, round(Correlation, 2), "")),
        color = "black", size = 3
    ) +
    scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
    theme_minimal() +
    theme(
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 45, vjust = 1)
    ) +
    labs(
        title = "Correlation Between Predictors and Responses",
        x = "",
        y = ""
    ) +
    coord_flip()
```

## Preparing training and testing data

```{r}
set.seed(9872345)
ml_split <- initial_split(ml_data, prop = split_prop)

ml_split
```

## Create a cross-validation set for later use

```{r}
ml_cv <- vfold_cv(training(ml_split))
```

## Create recipe

```{r}
# https://recipes.tidymodels.org/articles/Ordering.html

ml_recipes <- my_sapply(responses,function(y) {
    formula <- as.formula(paste(y, 
    paste(predictors[!predictors %in% c("lon","lat")], 
    collapse = "+"), sep = "~"))

    print(formula)

    ml_recipe <- recipe(formula, data = ml_data) %>%
        # Pre-processing steps
        ## Transform numeric predictors
        # step_YeoJohnson(all_numeric()) %>%
        # Handle unseen levels in categorical variables
        step_novel(all_nominal()) %>%
        ## Recode categorical variables
        step_dummy(all_nominal()) %>%
        ## Standardize all numeric columns
        # step_normalize(all_numeric()) %>%
        # Remove NAs
        step_naomit()

        return(ml_recipe)
})
```

# Create the model

```{r}
ml_model <-
    # specify that the model is a random forest
    rand_forest() %>%
    # Parameters to be tuned
    set_args(
        mtry = tune(),
        trees = tune(),
        min_n = tune()
    ) %>%
    # Select the modeling engine
    set_engine(engine = ml_engine, importance = "permutation") %>%
    # Select between regression and binary classification
    set_mode(ml_mode)
```

## Create a workflow

```{r}
ml_workflows <- my_sapply(names(ml_recipes), function(recipe) {
    ml_workflow <- workflow() %>%
        # add the recipe
        add_recipe(ml_recipes[[recipe]]) %>%
        # add the model
        add_model(ml_model)
})
```

## Tune parameters

```{r}
ntuning <- nrow(tuning_params)
nlimit <- 100
plan("future::multisession",
    workers = ifelse(ntuning < nlimit, ntuning, nlimit)
)

tic()
# extract results
ml_tune_results <- lapply(ml_workflows, function(workflow) {
    workflow %>%
        tune_grid(
            # Cross-validation data object
            resamples = ml_cv, 
            # Grid of hyperparameters
            grid = tuning_params,
            # Metrics to evaluate
            metrics = ml_metrics
        )
})
toc()

saveRDS(ml_tune_results, "rds_backups/ml_tune_results.rds")
```

## Plot tunned parameters

```{r}
# Re-import tuning results
ml_tune_results <- readRDS("rds_backups/ml_tune_results.rds")

# Generate and display plots for each element in ml_tune_results
lapply(ml_tune_results, function(x) {
    autoplot(x) + ggtitle("Tuning Results")
})
```

## Collect and evaluate results

```{r}
#| eval: true
ml_tune_results <- lapply(ml_tune_results, function(x) {
    select_best(x, metric = "rmse")
})

print(ml_tune_results)
```

## Finalize workflow

```{r}
#| eval: true
for (x in 1:length(ml_workflows)) {
    ml_workflows[[x]] <- finalize_workflow(
        ml_workflows[[x]],
        ml_tune_results[[x]]
    )
}
```

# Fit the final model to the training and testing data

```{r}
#| eval: true
ml_fits <- lapply(ml_workflows, function(workflow) {
    workflow %>%
        # fit on the training set and evaluate on test set
        last_fit(ml_split)
})
```

check results

```{r}
test_performance <- lapply(ml_fits, function(ml_fit) {
    ml_fit %>% collect_metrics()
})

names(test_performance) <- responses

test_performance <- bind_rows(test_performance, .id = "Response")

test_performance <- test_performance %>%
    select(Response, .metric, .estimate) %>%
    pivot_wider(names_from = .metric, values_from = .estimate)

test_performance

# rmse 0.5809
# rsq 0.5920
```

### Variable importance plots
```{r}
pacman::p_load(vip, patchwork, dplyr, ggplot2)

names(ml_fits) <- responses

extract_varimportance <- function(model, response, num_features = 15) {
    f_model <- extract_fit_parsnip(model)
    importance_scores <- vip(f_model$fit, num_features = num_features, geom = "col")$data
    importance_scores <- importance_scores %>%
        mutate(Response = response) # Add response variable for faceting
    return(importance_scores)
}

# Extract importance scores for all models and combine into a single dataframe
vip_data <- bind_rows(lapply(names(ml_fits), function(response) {
    extract_varimportance(ml_fits[[response]], response)
}))

# Plot variable importance with facets and merged x axes
vip_plot <- ggplot(vip_data, aes(x = reorder(Variable, Importance), y = Importance)) +
    geom_col(fill = "lightblue") +
    coord_flip() +
    theme_pubr() +
    theme(legend.position = "none") + # Remove the legend
    facet_wrap(~Response, scales = "free_x") + # Use scales = "free_y" to merge x axes
    labs(title = "", x = "", y = "")

# Save the plot
ggsave("figures/faceted_variable_importance_plots.png",
    plot = vip_plot, width = 20, height = 30, units = "cm", bg = "white"
)

vip_plot
```

#### Interactions
```{r}
pacman::p_load(hstats, ggpubr)

names(ml_fits) <- responses

plot_interactions <- function(model, fileid = NULL, background_data = NULL) {
    results <- list()
    model <- extract_workflow(model)
    hstat <- hstats(model, X = background_data)

    h2_stat <- hstat$h2$num / hstat$h2$denom

    plot <- plot(hstat) +
        theme_pubr() +
        annotate("text",
            x = Inf, y = -Inf, label = paste0("h² = ", round(h2_stat, 2)),
            hjust = 1.1, vjust = -0.5, size = 4, color = "blue"
        )

    results[["plot_norm"]] <- plot

    results[["plot_not_norm"]] <- plot(h2_pairwise(hstat, normalize = FALSE, squared = FALSE)) +
        theme_pubr()

    return(results)
}

# Loop over the models
library(parallel)

mcmapply(function(response) {
    res <- plot_interactions(
        model = ml_fits[[response]],
        fileid = response,
        background_data = ml_data %>% select(all_of(c(response, predictors)))
    )

    ggsave(plot = res[["plot_norm"]], filename = str_glue("figures/{response}_normalized_interactions.png"))

    ggsave(plot = res[["plot_not_norm"]], filename = str_glue("figures/{response}_interactions_strength.png"))
}, names(ml_fits), USE.NAMES = TRUE, SIMPLIFY = FALSE, mc.cores = parallel::detectCores() - 1)
```

#### Partial dependence plots

```{r}
pacman::p_load(vip)

names(ml_fits) <- responses

plot_partial_dependence <- function(model, features, fileid = NULL, background_data = NULL) {
    f_model <- extract_fit_parsnip(model)

    # Generate partial dependence data
    pd_data <- hstats::partial_dep(f_model$fit, v = features, X = background_data %>% select(all_of(features)))

    # Plot partial dependence
    pd_plot <- ggplot(pd_data, aes_string(x = features, y = "partial_dependence")) +
        geom_line(color = "blue") +
        theme_minimal() +
        labs(
            title = "",
            x = "",
            y = "Partial Dependence"
        )

    # Save the plot
    if (!is.null(fileid)) {
        ggsave(str_glue("figures/{fileid}_partial_dependence_{feature}.png"),
            plot = pd_plot, width = 15, height = 10, units = "cm", bg = "white"
        )
    }

    return(pd_plot)
}

# Loop over the models and features
sapply(names(ml_fits), USE.NAMES = T, simplify = F, function(response) {
    required_features <- c("maxlength", "Z")
    if (all(required_features %in% colnames(ml_data))) {
        plot_partial_dependence(ml_fits[[response]], features = required_features, fileid = response, background_data = ml_data)
    } else {
        warning(str_glue("Skipping {response}: Required features not found in data."))
    }
})
```



## Interpreting most important variables from random forest

-   Identify the threshold for North-South extent and CV-EVI.



# Results

## Figure 1

Ordination figure goes here.

```{r}
nmds_plot
```

## Figure XX

-Interpreting most important variables

```{r}
# Plot results
library(ggplot2)
plot_data <- data.frame(ns_length = data$ns_length, cv_evi = data$cv_evi, Fitted = fitted_values)

ggplot(plot_data, aes(x = ns_length, y = cv_evi)) +
    geom_point() +
    geom_line(aes(y = Fitted), color = "blue") +
    geom_vline(xintercept = breakpoints, linetype = "dashed", color = "red") +
    labs(title = "Segmented Regression", x = "North-South Length", y = "CV-EVI") +
    scale_x_log10() +
    ggpubr::theme_pubr()
```

# Discussion

\[Discussion content to be added.\]

Nature protection often has complex goals: Šumava National Park and Czech Switzerland National Park are prime examples of protected areas where protecting landscape character is almost as important as protecting habitats and biodiversity.
